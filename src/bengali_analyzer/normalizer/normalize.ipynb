{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pathlib\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "from bnunicodenormalizer import Normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 8192\n",
    "CHECKSUM_DIR = \"./checksum.json\"\n",
    "ERROR_DIR = \"./defective_assests/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_count = 0\n",
    "\n",
    "def get_error_dir():\n",
    "    return pathlib.Path().resolve().as_posix() + \"/\" + ERROR_DIR.split(\"/\")[-2] + \"/\"\n",
    "\n",
    "\n",
    "def get_file_name(asset_path):\n",
    "    return asset_path.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "def get_file_extension(asset_path):\n",
    "    return os.path.splitext(asset_path)[1]\n",
    "\n",
    "\n",
    "def generate_error_report(asset_path, line):\n",
    "    global error_count\n",
    "\n",
    "    error_count += 1\n",
    "    file_name = get_file_name(asset_path)\n",
    "\n",
    "    if not os.path.exists(ERROR_DIR):\n",
    "        os.makedirs(ERROR_DIR)\n",
    "\n",
    "    with open(ERROR_DIR + file_name, \"a\") as outfile:\n",
    "        outfile.write(line)\n",
    "\n",
    "\n",
    "def file_replace(tmp_path, asset_path):\n",
    "    if os.path.exists(tmp_path):\n",
    "        os.replace(tmp_path, asset_path)\n",
    "\n",
    "\n",
    "def generate_checksum(files):\n",
    "    checksums = {}\n",
    "\n",
    "    for asset_path in files:\n",
    "        with open(asset_path, \"rb\") as f:\n",
    "            file_hash = hashlib.blake2b()\n",
    "            while chunk := f.read(CHUNK_SIZE):\n",
    "                file_hash.update(chunk)\n",
    "\n",
    "        checksums[get_file_name(asset_path)] = file_hash.hexdigest()\n",
    "\n",
    "    return checksums\n",
    "\n",
    "\n",
    "def normalize_word(word):\n",
    "    bn_normalizer = Normalizer(allow_english=True)\n",
    "    normalized_token = bn_normalizer(word)\n",
    "\n",
    "    return normalized_token[\"normalized\"]\n",
    "\n",
    "\n",
    "def normalize_sentence(sentence):\n",
    "    words = sentence.strip().split(\" \")\n",
    "\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        sentence += str(normalize_word(word=word)) + \" \"\n",
    "\n",
    "    return sentence.strip()\n",
    "\n",
    "\n",
    "# Only csv and txt handled here\n",
    "def normalize_other(asset_path):\n",
    "    tmp_path = \"./tmp.txt\"\n",
    "\n",
    "    with open(asset_path, \"r\") as f:\n",
    "        lines = sorted(set(f.readlines()))\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            try:\n",
    "                line = normalize_sentence(sentence=line)\n",
    "\n",
    "                # after normalizing every line it is being written to tmp file\n",
    "                with open(tmp_path, \"a\") as f2:\n",
    "                    f2.writelines(line + \"\\n\")\n",
    "            except:\n",
    "                # print(\n",
    "                #     colored(\n",
    "                #         f\"ERROR: In line {i} of file {asset_path}, output: {line}\",\n",
    "                #         \"red\",\n",
    "                #     )\n",
    "                # )\n",
    "\n",
    "                generate_error_report(asset_path=asset_path, line=line)\n",
    "\n",
    "    # Replacing the original file after a successful normalization\n",
    "    file_replace(tmp_path=tmp_path, asset_path=asset_path)\n",
    "\n",
    "\n",
    "def normalize_json(asset_path):\n",
    "    tmp_path = \"./tmp.json\"\n",
    "\n",
    "    with open(asset_path, \"r\") as f:\n",
    "        data = json.dumps(json.load(f), ensure_ascii=False)\n",
    "\n",
    "        data = normalize_sentence(sentence=data)\n",
    "\n",
    "        # Writing to temporary json\n",
    "        with open(tmp_path, \"w\") as outfile:\n",
    "            outfile.write(data)\n",
    "\n",
    "    # Replacing the original file after a successful normalization\n",
    "    file_replace(tmp_path=tmp_path, asset_path=asset_path)\n",
    "\n",
    "\n",
    "def get_non_normalized_files(files, file_dir):\n",
    "    checksums = generate_checksum(files)\n",
    "\n",
    "    with open(CHECKSUM_DIR) as json_file:\n",
    "        original_checksums = json.load(json_file)\n",
    "\n",
    "    diff = [\n",
    "        c\n",
    "        for c in checksums\n",
    "        if c not in original_checksums or original_checksums[c] != checksums[c]\n",
    "    ]\n",
    "\n",
    "    return [file_dir + i for i in diff]\n",
    "\n",
    "\n",
    "def normalize_assets(file_dir, ignore_files=[]):\n",
    "    all_files = glob.glob(file_dir + \"*\")\n",
    "    supported_extensions = [\".csv\", \".CSV\", \".txt\", \".TXT\", \".json\", \".JSON\"]\n",
    "    files = [\n",
    "        files\n",
    "        for files in all_files\n",
    "        if get_file_name(files) not in ignore_files\n",
    "        and get_file_extension(files) in supported_extensions\n",
    "    ]\n",
    "\n",
    "    non_normalized_files = get_non_normalized_files(files=files, file_dir=file_dir)\n",
    "\n",
    "    if len(non_normalized_files):\n",
    "        print(\n",
    "            colored(\n",
    "                \"One or multiple assets has been changed\\nOnly '*.csv', '*.txt' and '*.json' files will be normalized\\n\\nNormalizing those assets, please wait...\",\n",
    "                \"yellow\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for asset_path in tqdm(non_normalized_files):\n",
    "            if get_file_extension(asset_path) in [\".json\", \".JSON\"]:\n",
    "                normalize_json(asset_path)\n",
    "            else:\n",
    "                normalize_other(asset_path)\n",
    "\n",
    "        # Update file's checksum\n",
    "        new_checksum = generate_checksum(files=files)\n",
    "        with open(CHECKSUM_DIR, \"w\") as outfile:\n",
    "            json.dump(new_checksum, outfile)\n",
    "\n",
    "        if error_count:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{error_count} errors occured\\nCheck {get_error_dir()} to identify which type of patterns we can't currently normalize!!!\\n\",\n",
    "                    \"red\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(colored(\"Normalization completed ðŸ¥³ðŸ¥³ðŸ¥³\\n\", \"green\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mOne or multiple assets has been changed\n",
      "Only '*.csv', '*.txt' and '*.json' files will be normalized\n",
      "\n",
      "Normalizing those assets, please wait...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:02<00:00, 122.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mNormalization completed ðŸ¥³ðŸ¥³ðŸ¥³\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IGNORE_FILES = []\n",
    "FILE_DIR = \"../assets_tmp/\"\n",
    "\n",
    "normalize_assets(file_dir=FILE_DIR, ignore_files=IGNORE_FILES)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b47640ec1daf6351a50a2ef50da74bad4769d96980e20cb22b699ad5af89322f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
